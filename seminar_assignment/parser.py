from nltk.corpus import treebank
from nltk.tag import DefaultTagger
from nltk.tag import UnigramTagger
from nltk.tag import BigramTagger
from nltk.tag import TrigramTagger
import nltk
import re
from nltk.tokenize import *	
from nltk.corpus import names
from collections import Counter
import requests
from os import listdir
from os.path import isfile, join
from itertools import combinations
from nltk.corpus.reader import WordListCorpusReader

#parse tagged class
#found it way easier and cleaner to use a separate class for this
class Parse:
	def __init__(self,folder_name,file_name):
		self.folder_name = folder_name
		self.file_name   = file_name

	#return tuple of lists
	#eg: (['abstract','speaker',...],['asd', 'asdads'...])
	#basically same idea as dictionaries but keep them ordered (maybe not the best solution)
	def find_info_type(self):
		type_list    = [] #list of all types('abstract','speaker')
		content_list = [] #list with content
		reader       = WordListCorpusReader(self.folder_name,[self.file_name])
		all_words    = reader.words()

		#is the mail a proper one?
		if(all_words == []):
			return ([],[])

		#append the first tag of the mail ex:<0.1....>
		type_list.append("")
		content_list.append(all_words[0])

		for w in all_words[1:]:
			#search for pattern like "Abstract: ..."
			type = re.search('^(\w+)(:)',w)
			
			#using group functionality to split the topic and content
			if(type != None):
				type_list.append(type.group(1))
				content = re.search('^(\w+:)(.*)',w)
				content_list.append(content.group(2))
			
			#not the best way to add the \n splitted content but...
			elif(len(content_list)>0):
				last_element     = content_list[-1]
				extra_content    = w
				last_element     = last_element + "\n" + extra_content
				content_list[-1] = last_element
		
		#if the type_list[0] will be 'abstract' then content_list[0] will be the abstract content 
		return (type_list, content_list)


#get all files from given path
def get_file_paths(path=''):
	only_files = [f for f in listdir(path) if isfile(join(path,f)) and "txt" in f]
	return only_files


#return the tuple of list of tuples generated by parse class
#for all txt files which are in the given path
#and the name of files in order to store them and tag them
def parse_data(path=''):
	files_list = get_file_paths(path)
	files_list = sorted(files_list)
	all_mails  = []

	for f in files_list:
		p = Parse(path,f)
		all_mails.append(p.find_info_type())
	
	return (all_mails, files_list)


#return it like: {"speaker":["asdads","adasd"...],"location":["ads","ads"...]...}
#in order to use it for untagged data
def get_training_sents(tuple_of_lists):
	
	all_tags       = []          #all possible tags
	training_sents = [[]]
	
	#sentence and paragraph are not important for the purpose of this parsing
	desired_tags = ["<stime>", 
	                "<etime>", 
	                "<speaker>", 
	                "<location>"
	               ]

	for element in tuple_of_lists:	
		#only abstract content needed
		if ("Abstract" not in element[0]):
			continue

		abstract = element[1][element[0].index("Abstract")]

		#regex for all tags
		tag = "(?i)<\/?\w+((\s+\w+(\s*=\s*(?:\".*?\"|'.*?'|[^'\">\s]+))?)+\s*|\s*)\/?>" 

		#finding all tags
		for match in re.finditer(tag, abstract):
			all_tags = all_tags + [(match.group())]
		
		#removing the closing tags and sentence and paragraph tags 
		all_tags = [item for item in all_tags if not "</" in item and item in desired_tags]
		
		#removing duplicates
		set_all_tags = set(all_tags)
		all_tags     = list(set_all_tags)
		abstract     = abstract.replace('\n','')
		
		tagged_list = []		
		
		for t2 in all_tags:
			closet2     = t2[:1]+"/" + t2[1:]
			reg         = t2 + r'.*?' + closet2
			content     = re.findall(reg, abstract)
			tagged_list = tagged_list + content
		
		training_list   = []
		
		for tagged_element in tagged_list:
			s             = re.search(r'^<(\w*?)>',tagged_element)
			delete_tags   = re.sub(r'<.*?>', '', tagged_element)
			training_list = training_list + [(delete_tags, s.group(1))]		
		
		training_sents.append(training_list)

	#define dict: (tag, list of values) (eg:('stime','3:30 pm'))
	tag_dict             = {}
	tag_dict['stime']    = []
	tag_dict['etime']    = []
	tag_dict['location'] = []
	tag_dict['speaker']  = []

	#populating the lists and finally the dictionary
	for abstr1 in training_sents:
		for pair in abstr1:
			tag_dict[pair[1]].append(pair[0])
	return tag_dict

